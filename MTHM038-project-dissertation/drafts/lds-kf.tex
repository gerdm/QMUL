\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}

\title{Kalman Filters: an introduction}
\author{Gerardo Durán Martín}
\begin{document}
\maketitle

\section{Introduction}

Dynamical systems are models that capture the time evolution of a system. A discrete-time linear dynamical system is a dynamical system in which an $M$-dimensional vector ${\bf x}_n$ evolves in the form.

\begin{equation}
	{\bf x}_{n+1} = {\bf A x}_n
\end{equation}

An example of a linear dynamical system is...


Suppose $\{{\bf z}_t\}_t$ is a signal coming from an unknown source and $\{{\bf x_t}\}$ are observed values derived from ${\bf z}_t$. If the system is linear and deterministic we write the equations that govern such system in the form

\begin{align*}
	{\bf z}_{n+1} &= {\bf A} {\bf z}_{n}\\
	{\bf x}_{n+1} &= {\bf C} {\bf z}_{n+1}
\end{align*}

In this model, we assume the existence of an unobserved signal ${\bf z}_t$ that evolves over time according to the matrix $\bf A$. In some settings, however, it is not the case that the signal we are trying to find behaves deterministically. To take into account the uncertainty of the system, we assume that that both the underlying signal and the observed signal are corrupted by some noise. Under this scenario, we modify our previous system as follows:
 

\begin{align*}
	{\bf z}_{n+1} &= {\bf A} {\bf z}_{n} + \boldsymbol\varepsilon_n\\
	{\bf x}_{n+1} &= {\bf C} {\bf z}_{n+1} + \boldsymbol\varphi_n
\end{align*}

The noise terms are assumed to be normally distributed with zero mean and covariance matrix $\boldsymbol{\Gamma}$, and $\boldsymbol{\Sigma}$.



\begin{align}
	\boldsymbol\varepsilon_n &\sim \mathcal{N}({\bf 0}, \boldsymbol\Gamma)\\
	\boldsymbol\varphi & \sim \mathcal{N}({\bf 0}, \boldsymbol\Sigma)
\end{align}

We notice that the model has a likelihood given by

\begin{equation}
	p({\bf X}, {\bf Z}) = p({\bf z}_1)\prod_{n=2}^N p({\bf z}_n\vert {\bf z}_{n-1})\prod_{n=1}^N p({\bf x}_n\vert {\bf z}_n)
\end{equation}

Our purpose is to find parameters $\boldsymbol\theta = \{{\bf A}, {\bf C}, \boldsymbol\Gamma, \boldsymbol\Sigma\}$ that best represent the data. This is usually done maximising the likelihood of the data with respect to the parameters $\boldsymbol{\theta}$. Maximising the likelihood of this model, however, is not straightforward. This is because the only observed data we have is given by the dataset $\{{\bf x}_t\}_t$. 


One approach of finding such parameters is to make use of the EM algorithm: an iterative approach to maximise the likelihood of the complete-data log-likelihood as follows

\begin{align}
	\boldsymbol{\theta}^\text{new} &= \argmax{\boldsymbol{\theta}} \mathbb{E}_{{\bf Z}\vert {\bf X}, \boldsymbol{\theta}^\text{old}}[\log p({\bf X}, {\bf Z}\vert \boldsymbol\theta)]
\end{align}

The computation of the posterior probability of latent variables $p({\bf Z}\vert {\bf X}, \boldsymbol\theta)$ is called the E-step. The  of the complete-data log-likelihood with respect to the posterior latent variables is called the M-step.


\subsection{The E-step}
To make use of the E-step, it is helpful to distinguish which elements depend on ${\bf z}_n$. Consider

\begin{align}
	\log p({\bf X}, {\bf Z}\vert \boldsymbol\theta) &= \log p({\bf z}_1) + \sum_{n=2}^N \log p({\bf z}_n\vert {\bf z}_{n-1}) + \sum_{n=1}^N \log p({\bf x}_n\vert {\bf z}_n)\\
	   &=\frac{1}{2}\log\vert{\bf V}_0^{-1}\vert - \frac{1}{2}({\bf z}_1 - \boldsymbol\mu_0)^T{\bf V}_0^{-1}({\bf z}_1 - \boldsymbol\mu_0) \nonumber \\
	   &\hspace{1cm}+ \sum_{n=2}^N \frac{1}{2}\log\vert\boldsymbol\Gamma^{-1}\vert - \frac{1}{2}({\bf z}_n - {\bf A z}_{n-1})^T\boldsymbol{\Gamma}^{-1}({\bf z}_n - {\bf A z}_{n-1}) \nonumber \\
	   &\hspace{1cm}+\sum_{n=1}^N \frac{1}{2}\log\vert\boldsymbol\Sigma\vert -\frac{1}{2}({\bf x}_n - {\bf C z}_n)^T \boldsymbol{\Sigma}^{-1}({\bf x}_n - {\bf C z}_n) + \text{const.}\\
	   &= \frac{1}{2}\log \vert
	  {\bf V}_0^{-1}\vert -\frac{1}{2}\left[\text{Tr}\left({\bf z}_1 {\bf z}_1^T {\bf V}_0^{-1}\right) -2 {\bf z}_1{\bf V}_0^{-1}\boldsymbol\mu_0 + \boldsymbol\mu_0 {\bf V}_0^{-1}\boldsymbol\mu_0^T\right] \nonumber \\
	  &\hspace{1cm}+ \frac{N-1}{2}\log\vert\boldsymbol\Gamma^{-1}\vert + \frac{N}{2}\log\vert \boldsymbol\Sigma^{-1}\vert \nonumber \\
	  &\hspace{1cm}-\frac{1}{2} \sum_{n=2}^{N}\text{Tr}\left({\bf z}_n{\bf z}_n^T\boldsymbol\Gamma^{-1} -2 {\bf z}_{n-1}{\bf z}_n^T{\bf A}\boldsymbol\Gamma^{-1} +  {\bf z}_{n-1}{\bf z}_{n-1}^T{\bf A}\boldsymbol\Gamma^{-1}{\bf A}\right) \nonumber\\
	  &\hspace{1cm}-\frac{1}{2}\sum_{n=1}^N\left[{\bf x}_n^T\boldsymbol{\Sigma}^{-1}{\bf x}_n-2{\bf z}_n^T\boldsymbol\Sigma^{-1}{\bf x}_n + \text{Tr}({\bf z}_n{\bf z}_n^T{\bf C}\boldsymbol\Sigma^{-1}{\bf C})\right] \nonumber\\
	  &\hspace{1cm} + \text{const.}
\end{align}

Where const. are the terms that do not depend on any of the terms in $\boldsymbol{\theta}$. From this last equation, we note that the expectation with respect to the posterior distribution comes only in the form $\mathbb{E}[{\bf z}_n]$, $\mathbb{E}[{\bf z}_n{\bf z}_{n}^{T}]$, and  $\mathbb{E}[{\bf z}_n{\bf z}_{n-1}^{T}]$. We obtain:

\begin{align}
	Q(\boldsymbol\theta, \boldsymbol\theta^\text{old}) &= \frac{1}{2}\log \vert
	  {\bf V}_0^{-1}\vert -\frac{1}{2}\left[\text{Tr}\left(\mathbb{E}\left[{\bf z}_1 {\bf z}_1^T\right] {\bf V}_0^{-1}\right) -2 \mathbb{E}\left[{\bf z}_1\right]{\bf V}_0^{-1}\boldsymbol\mu_0 + \boldsymbol\mu_0 {\bf V}_0^{-1}\boldsymbol\mu_0^T\right] \nonumber \\
	  &\hspace{1cm}+ \frac{N-1}{2}\log\vert\boldsymbol\Gamma^{-1}\vert + \frac{N}{2}\log\vert \boldsymbol\Sigma^{-1}\vert \nonumber \\
	  &\hspace{1cm}-\frac{1}{2} \sum_{n=2}^{N}\text{Tr}\left(\mathbb{E}\left[{\bf z}_n{\bf z}_n^T\right]\boldsymbol\Gamma^{-1} -2\mathbb{E}\left[ {\bf z}_{n-1}{\bf z}_n^T\right]{\bf A}\boldsymbol\Gamma^{-1} + \mathbb{E}\left[{\bf z}_{n-1}{\bf z}_{n-1}^T\right]{\bf A}\boldsymbol\Gamma^{-1}{\bf A}\right) \nonumber\\
	  &\hspace{1cm}-\frac{1}{2}\sum_{n=1}^N\left[{\bf x}_n^T\boldsymbol{\Sigma}^{-1}{\bf x}_n-2\mathbb{E}\left[{\bf z}_n^T\right]\boldsymbol\Sigma^{-1}{\bf x}_n + \text{Tr}(\mathbb{E}\left[{\bf z}_n{\bf z}_n^T\right]{\bf C}\boldsymbol\Sigma^{-1}{\bf C})\right] \nonumber\\
	  &\hspace{1cm} + \text{const.}
\end{align}


Our next step is to compute the expected values of the latent variables. We begin by noting that 

\begin{align}
	\mathbb{E}[{\bf z}_n] &= \int_{{\bf z}_1}\cdots \int_{{\bf z}_N} {\bf z}_n p({\bf Z}\vert {\bf X}) d{\bf z}_1 \cdots {\bf z}_N \nonumber \\
\end{align}
%%to-do: Show that the E-step can be decomposed into a product of two terms


\begin{equation}
	\gamma({\bf z}_n) = \hat\alpha({\bf z}_n)\hat\beta({\bf z}_n).
\end{equation}

The term $\hat\alpha({\bf z}_n)$ is called the $\alpha$-forward message passing for a linear dynamical system or \textbf{Kalman filter equation}.  The term $\hat\beta({\bf z}_n)$ is called the $\beta$-backward message passing of a linear dynamical system or \textbf{Kalman smoother equation}. Intuitively, $\hat\alpha({\bf z}_n)$ represents the information that the history of the data has on the $n$th observation; $\hat\beta({\bf z}_n)$ represents how the current known value affects the future behaviour of the system.

\end{document}