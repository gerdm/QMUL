\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newcommand{\argmax}[1]{\underset{#1}{\operatorname{arg}\,\operatorname{max}}\;}
\newtheorem{proposition}{Proposition}[section]

\title{Kalman Filters: an introduction}
\author{Gerardo Durán Martín}
\begin{document}
\maketitle

\section{Introduction}

Dynamical systems are models that capture the time evolution of a system. A discrete-time linear dynamical system is a dynamical system in which an $M$-dimensional vector ${\bf x}_n$ evolves in the form.

\begin{equation}
	{\bf x}_{n+1} = {\bf A x}_n
\end{equation}

An example of a linear dynamical system is...


Suppose $\{{\bf z}_t\}_t$ is a signal coming from an unknown source and $\{{\bf x_t}\}$ are observed values derived from ${\bf z}_t$. If the system is linear and deterministic we write the equations that govern such system in the form

\begin{align*}
	{\bf z}_{n+1} &= {\bf A} {\bf z}_{n}\\
	{\bf x}_{n+1} &= {\bf C} {\bf z}_{n+1}
\end{align*}

In this model, we assume the existence of an unobserved signal ${\bf z}_t$ that evolves over time according to the matrix $\bf A$. In some settings, however, it is not the case that the signal we are trying to find behaves deterministically. To take into account the uncertainty of the system, we assume that that both the underlying signal and the observed signal are corrupted by some noise. Under this scenario, we modify our previous system as follows:
 

\begin{align*}
	{\bf z}_{n+1} &= {\bf A} {\bf z}_{n} + \boldsymbol\varepsilon_n\\
	{\bf x}_{n+1} &= {\bf C} {\bf z}_{n+1} + \boldsymbol\varphi_n
\end{align*}

The noise terms are assumed to be normally distributed with zero mean and covariance matrix $\boldsymbol{\Gamma}$, and $\boldsymbol{\Sigma}$.



\begin{align}
	\boldsymbol\varepsilon_n &\sim \mathcal{N}({\bf 0}, \boldsymbol\Gamma)\\
	\boldsymbol\varphi & \sim \mathcal{N}({\bf 0}, \boldsymbol\Sigma)
\end{align}

We notice that the model has a likelihood given by

\begin{equation}
	p({\bf X}, {\bf Z}) = p({\bf z}_1)\prod_{n=2}^N p({\bf z}_n\vert {\bf z}_{n-1})\prod_{n=1}^N p({\bf x}_n\vert {\bf z}_n)
\end{equation}

Our purpose is to find parameters $\boldsymbol\theta = \{{\bf A}, {\bf C}, \boldsymbol\Gamma, \boldsymbol\Sigma\}$ that best represent the data. This is usually done maximising the likelihood of the data with respect to the parameters $\boldsymbol{\theta}$. Maximising the likelihood of this model, however, is not straightforward. This is because the only observed data we have is given by the dataset $\{{\bf x}_t\}_t$. 


One approach of finding such parameters is to make use of the EM algorithm: an iterative approach to maximise the likelihood of the complete-data log-likelihood as follows

\begin{align}
	\boldsymbol{\theta}^\text{new} &= \argmax{\boldsymbol{\theta}} \mathbb{E}_{{\bf Z}\vert {\bf X}, \boldsymbol{\theta}^\text{old}}[\log p({\bf X}, {\bf Z}\vert \boldsymbol\theta)]\\
	&= \argmax{\boldsymbol{\theta}} Q(\boldsymbol\theta, \boldsymbol\theta^\text{old})
\end{align}

Where we have defined $Q(\boldsymbol\theta, \boldsymbol\theta^\text{old}) := \mathbb{E}_{{\bf Z}\vert {\bf X}, \boldsymbol{\theta}^\text{old}}[\log p({\bf X}, {\bf Z}\vert \boldsymbol\theta)]$.The computation of the posterior probability of latent variables $p({\bf Z}\vert {\bf X}, \boldsymbol\theta)$ is called the E-step. The maximisation of the complete-data log-likelihood with respect to the posterior latent variables is called the M-step.


\subsection{The E-step}
To make use of the E-step, it is helpful to distinguish which elements depend on ${\bf z}_n$. Consider

\begin{align}
	\log p({\bf X}, {\bf Z}\vert \boldsymbol\theta) &= \log p({\bf z}_1) + \sum_{n=2}^N \log p({\bf z}_n\vert {\bf z}_{n-1}) + \sum_{n=1}^N \log p({\bf x}_n\vert {\bf z}_n)\\
	   &=\frac{1}{2}\log\vert{\bf V}_0^{-1}\vert - \frac{1}{2}({\bf z}_1 - \boldsymbol\mu_0)^T{\bf V}_0^{-1}({\bf z}_1 - \boldsymbol\mu_0) \nonumber \\
	   &\hspace{1cm}+ \sum_{n=2}^N \frac{1}{2}\log\vert\boldsymbol\Gamma^{-1}\vert - \frac{1}{2}({\bf z}_n - {\bf A z}_{n-1})^T\boldsymbol{\Gamma}^{-1}({\bf z}_n - {\bf A z}_{n-1}) \nonumber \\
	   &\hspace{1cm}+\sum_{n=1}^N \frac{1}{2}\log\vert\boldsymbol\Sigma\vert -\frac{1}{2}({\bf x}_n - {\bf C z}_n)^T \boldsymbol{\Sigma}^{-1}({\bf x}_n - {\bf C z}_n) + \text{const.}\\
	   &= \frac{1}{2}\log \vert
	  {\bf V}_0^{-1}\vert -\frac{1}{2}\left[\text{Tr}\left({\bf z}_1 {\bf z}_1^T {\bf V}_0^{-1}\right) -2 {\bf z}_1{\bf V}_0^{-1}\boldsymbol\mu_0 + \boldsymbol\mu_0 {\bf V}_0^{-1}\boldsymbol\mu_0^T\right] \nonumber \\
	  &\hspace{1cm}+ \frac{N-1}{2}\log\vert\boldsymbol\Gamma^{-1}\vert + \frac{N}{2}\log\vert \boldsymbol\Sigma^{-1}\vert \nonumber \\
	  &\hspace{1cm}-\frac{1}{2} \sum_{n=2}^{N}\text{Tr}\left({\bf z}_n{\bf z}_n^T\boldsymbol\Gamma^{-1} -2 {\bf z}_{n-1}{\bf z}_n^T{\bf A}\boldsymbol\Gamma^{-1} +  {\bf z}_{n-1}{\bf z}_{n-1}^T{\bf A}\boldsymbol\Gamma^{-1}{\bf A}\right) \nonumber\\
	  &\hspace{1cm}-\frac{1}{2}\sum_{n=1}^N\left[{\bf x}_n^T\boldsymbol{\Sigma}^{-1}{\bf x}_n-2{\bf z}_n^T\boldsymbol\Sigma^{-1}{\bf x}_n + \text{Tr}({\bf z}_n{\bf z}_n^T{\bf C}\boldsymbol\Sigma^{-1}{\bf C})\right] \nonumber\\
	  &\hspace{1cm} + \text{const.}
\end{align}

Where const. are the terms that do not depend on $\boldsymbol{\theta}$. From this last equation, we note that the expectation with respect to the posterior distribution comes only in the form of the expectations $\mathbb{E}[{\bf z}_n]$, $\mathbb{E}[{\bf z}_n{\bf z}_{n}^{T}]$, and  $\mathbb{E}[{\bf z}_n{\bf z}_{n-1}^{T}]$. We obtain:

\begin{align}
	Q(\boldsymbol\theta, \boldsymbol\theta^\text{old}) &= \frac{1}{2}\log \vert
	  {\bf V}_0^{-1}\vert -\frac{1}{2}\left[\text{Tr}\left(\mathbb{E}\left[{\bf z}_1 {\bf z}_1^T\right] {\bf V}_0^{-1}\right) -2 \mathbb{E}\left[{\bf z}_1\right]{\bf V}_0^{-1}\boldsymbol\mu_0 + \boldsymbol\mu_0 {\bf V}_0^{-1}\boldsymbol\mu_0^T\right] \nonumber \\
	  &\hspace{1cm}+ \frac{N-1}{2}\log\vert\boldsymbol\Gamma^{-1}\vert + \frac{N}{2}\log\vert \boldsymbol\Sigma^{-1}\vert \nonumber \\
	  &\hspace{1cm}-\frac{1}{2} \sum_{n=2}^{N}\text{Tr}\left(\mathbb{E}\left[{\bf z}_n{\bf z}_n^T\right]\boldsymbol\Gamma^{-1} -2\mathbb{E}\left[ {\bf z}_{n-1}{\bf z}_n^T\right]{\bf A}\boldsymbol\Gamma^{-1} + \mathbb{E}\left[{\bf z}_{n-1}{\bf z}_{n-1}^T\right]{\bf A}\boldsymbol\Gamma^{-1}{\bf A}\right) \nonumber\\
	  &\hspace{1cm}-\frac{1}{2}\sum_{n=1}^N\left[{\bf x}_n^T\boldsymbol{\Sigma}^{-1}{\bf x}_n-2\mathbb{E}\left[{\bf z}_n^T\right]\boldsymbol\Sigma^{-1}{\bf x}_n + \text{Tr}(\mathbb{E}\left[{\bf z}_n{\bf z}_n^T\right]{\bf C}\boldsymbol\Sigma^{-1}{\bf C})\right] \nonumber\\
	  &\hspace{1cm} + \text{const.}
\end{align}

Before computing the expected values of the latent variables, it is useful to find the posterior distributions of the latent variables $\gamma({\bf z}_n) := p({\bf z}_n\vert {\bf X})$, and $\xi({\bf z}_{n-1}, {\bf z}_{n}) := p({\bf z}_{n-1}, {\bf z}_n\vert {\bf X})$. %why is it useful?

\begin{proposition}
	The term $\gamma({\bf z}_n)$ can be written as a product of the joint probabilities of the first $n$ observations and ${\bf z}_n$, and the conditional probability of the observations that follow ${\bf z}_n$ conditional on ${\bf z}_n$.
\end{proposition}

\begin{proof}
Consider  $\gamma({\bf z}_n)$ and equation \ref{gm:1} from proposition \ref{prop:graphical-models-separation}. We have
\begin{align}
	\gamma({\bf z}_n) &= p({\bf z}_n \vert {\bf X})\\
					  &= \frac{1}{p({\bf X})}p({\bf z}_n)p({\bf X} \vert {\bf z}_n)\\
					  &= \frac{1}{p({\bf X})} p({\bf z}_n) p({\bf x}_1, \ldots, {\bf x}_n\vert {\bf z}_n) p({\bf x}_{n+1}, \ldots, {\bf x}_N\vert {\bf z}_n)\\
					  &= \frac{1}{p({\bf X})}p({\bf x}_1, \ldots, {\bf x}_n, {\bf z}_n) p({\bf x}_{n+1}, \ldots, {\bf x}_N\vert {\bf z}_n)\\
					  &= \frac{1}{p({\bf X})}\alpha({\bf z}_n)\beta({\bf z}_n)
\end{align}

Where we have denoted $\alpha({\bf z}_n) := p({\bf x}_1, \ldots, {\bf x}_n, {\bf z}_n)$ as the joint probability of the observed data up to $n$, and $\beta({\bf z}_n) := p({\bf x}_{n+1}, \ldots, {\bf x}_N\vert {\bf z}_n)$ as the probability of all data that follows the $n$-th observation, conditional on the $n$-th latent variable ${\bf z}_n$.
\end{proof}

% Here we show that the xi can also be written in terms of alpha and beta
\begin{proposition}
	The term $\beta({\bf z}_n)$ can be written as a 
\end{proposition}

% Hence, to make sense of gamma and xi we only need to find values of alpha and beta
% 	* We show that alpha can be represented as a recursive formula
%  	* We show that beta can be represented as a recursive formula
% We argue that alpha and beta values can become small very quick, so we require another way to compute its values
% Introduce alpha hat and beta hat: show that they can be written
% Show that alpha hat is a normal distribution
%	* Derive the Kalman Filter Equations
% Show that gamma is also a normal distribution


\begin{equation}
	\gamma({\bf z}_n) = \hat\alpha({\bf z}_n)\hat\beta({\bf z}_n).
\end{equation}

The term $\hat\alpha({\bf z}_n)$ is called the $\alpha$-forward message passing for a linear dynamical system or \textbf{Kalman filter equation}.  The term $\hat\beta({\bf z}_n)$ is called the $\beta$-backward message passing of a linear dynamical system or \textbf{Kalman smoother equation}. Intuitively, $\hat\alpha({\bf z}_n)$ represents the information that the history of the data has on the $n$th observation; $\hat\beta({\bf z}_n)$ represents how the current known value affects the future behaviour of the system.


\section{Graphical Models}
In this section we present some useful proposition of graphical models.

\begin{proposition}\label{prop:graphical-models-separation}
	Let ${\bf Z} = \{{\bf z}_n\}_n$ be a set of latent random variables, and ${\bf X} = \{{\bf x}_n\}_n$ a set of observed variables with complete-data likelihood $({\bf Z}, {\bf X})$ given by the LDS model. Then, the following conditions hold true
	\begin{align}
		p({\bf X}\vert{\bf z}_n) &= p({{\bf x}_1, \ldots, {\bf x}_n \vert {\bf z}_n})p({{\bf x}_{n+1}, \ldots, {\bf x}_N \vert {\bf z}_n}) \label{gm:1}\\
		p()
	\end{align}
\end{proposition}

\begin{proof}
	\texttt{to-do: show by D-separation or explicitly?}
\end{proof}

\end{document}